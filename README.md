ğŸ§© Contrastive Learning MLOps Pipeline - IN PROGRESS

End-to-End Multi-Stage Self-Supervised Learning Pipeline built with Hydra, MLflow, and PyTorch Lightning.
Designed for modular experimentation, reproducibility, and scalable model development.

ğŸš€ Overview

This project implements a multi-stage visual contrastive learning pipeline, integrating state-of-the-art self-supervised learning (SSL) techniques such as DINOv2, Linear Probe evaluation, and multi-modal CLIP-style alignment.
The architecture follows modern MLOps best practices, combining Hydra configuration management, MLflow experiment tracking, and PyTorch Lightning for modular and reproducible deep learning workflows.

ğŸ§  Pipeline Stages
Stage	Description	Output
Stage 1 â€” DINOv2 Pretraining	Self-supervised representation learning on ImageNet-100.	Vision backbone with strong features.
Stage 2 â€” Linear Probe	Train a linear classifier on frozen DINOv2 embeddings to measure representation quality.	Linear evaluation accuracy.
Stage 3 â€” CLIP Training	Multi-modal contrastive alignment between image and text features.	Unified vision-language embedding space.
Stage 4 â€” Demo / Retrieval	Launch retrieval demo for visual-semantic search.	End-to-end inference pipeline.

Each stage runs independently and can be resumed, skipped, or reconfigured dynamically through the unified Hydra + MLflow system.

âš™ï¸ Technical Stack
Category	Technologies
Core Framework	PyTorch â€¢ PyTorch Lightning â€¢ torchvision
Experiment Management	Hydra â€¢ MLflow â€¢ YAML-based config groups
Architecture	DINOv2 â€¢ Linear Probe â€¢ CLIP
Data Handling	Hugging Face Datasets (ImageNet-100)
Pipeline Control	Hydra Compose + Dynamic Overrides
MLOps	MLflow Tracking Server â€¢ Artifact logging â€¢ Stage-level skip via MLflow run state
Utilities	tqdm â€¢ omegaconf â€¢ pyyaml â€¢ pathlib
ğŸ§© Features

ğŸ”„ Multi-Stage Pipeline â€” DINOv2 â†’ Linear Probe â†’ CLIP â†’ Demo

âš™ï¸ Hydra Configuration System â€” dynamic config composition & inheritance

ğŸ“Š MLflow Integration â€” full tracking of parameters, metrics, and artifacts

ğŸ§  Idempotent Stage Execution â€” skip completed stages using MLflow run status

ğŸ§© Lightning Training Framework â€” clean modular structure and reproducibility

â˜ï¸ Scalable & Extensible â€” ready for multi-GPU, Colab, or cloud training

ğŸ§­ Project Structure
contrastive-learning/
â”‚
â”œâ”€â”€ main.py                    # Hydra + MLflow pipeline controller
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.yaml             # Global pipeline configuration
â”‚   â””â”€â”€ stages/
â”‚       â”œâ”€â”€ dinov2.yaml
â”‚       â”œâ”€â”€ linear_probe.yaml
â”‚       â”œâ”€â”€ clip.yaml
â”‚       â””â”€â”€ demo.yaml
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train_dinov2.py
â”‚   â”œâ”€â”€ train_linear_probe.py
â”‚   â”œâ”€â”€ train_clip.py
â”‚   â””â”€â”€ train_demo.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ tool.py
â””â”€â”€ requirements.txt

ğŸ§® Example Run
# Run full pipeline
python main.py

# Or run only one stage
python main.py stages=clip

# Override parameters dynamically
python main.py stages=dinov2 train.epochs=50 train.batch_size=128


Hydra automatically saves merged configs under:

outputs/<timestamp>/.hydra/config.yaml


MLflow automatically logs:

Parameters (epochs, lr, batch_size)

Metrics (loss, accuracy)

Artifacts (checkpoints, logs)

Run status (FINISHED / FAILED)


ğŸ“Š MLflow UI

To visualize all experiments:

mlflow ui

Then open: http://127.0.0.1:5000


ğŸ§  Summary

This project demonstrates how a modern machine learning engineer can architect a scalable, modular, and fully reproducible deep learning pipeline,
unifying model development and MLOps practices â€” from configuration and training, to logging, evaluation, and deployment.